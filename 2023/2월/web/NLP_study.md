## 자연어처리

텍스트 분석 수행 프로세스

1. 텍스트 사전 준비작업 ( 텍스트 전처리 ) : 텍스트를 피처로 만들기 전에 미리 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미 없는 단어 ( Stop Word ) 제거 작업, 어근 추출 ( Stemming / Lemmatization ) 등의 텍스트 정규화 작업을 수행하는 것을 말한다.

2. 피처 벡터화 / 추출 : 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당한다. 대표적인 방법으로는 BOW 와 Word2Vec이 있으며 BOW는 대표적으로 Count 기반과 TF-IDF 기반 벡터화가 있다.

3. ML 모델 수립 및 학습/예측/평가 : 피처 벡터화된 데이터 세트에 ML 모델을 적용해 학습/예측 및 평가를 수행한다.

## 클렌징
- 텍스트에서 분석에 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업 ( ex. HTML, XML 태그나 특정 기호 등을 사전에 제거 )

## 텍스트 토큰화
### 문장 토큰화

- 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적이다. 또한 정규 표현식에 따른 문장 토큰화도 가능하다.

## 스톱 워드 제거
- 스톱 워드 ( Stop Word )는 분석에 큰 의미가 없는 단어를 지칭한다.

## Stemming과 Lemmatization
문법적으로 또는 의미적으로 변화하는 단어의 원형을 찾는 것

- Stemming ( 어간 추출 ) : 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있다. ( Porter, Lancaster, Snowball Stemmer )
- Lemmatization ( 표제어 추출 ) : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾아준다. ( WordNetLemmatizer )


## 사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TfidVectorizer
### CountVectorizer, TfidVectorizer의 파라미터

- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외, 정수 값을 가지면 전체 문서에 걸쳐 n개 이하로 나타나는 단어만 피처로 추출하고 부동 소수점 값을 가지면 전체 문서에 걸쳐 빈도수% 까지의 단어만 피처로 추출한다.
- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외
- max_features : 높은 빈도를 가지는 단어 순으로 몇 개를 추출할 건지 입력
- stop_words : english로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외한다.
- n_gram_range : BOW 모델이 문맥의 의미를 반영하지 못한다는 단점을 극복하기 위해 n_gram 범위를 설정한다. 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정
- analyzer : 피처 추출을 수행한 단위를 지정한다. default = 'word'
- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정합니다. default = '\b\w\w+\b'
- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용한다.


## CountVectorizer 클래스를 이용한 피처 벡터화 방법

- 사전 데이터 가공 : 영어의 경우 모든 문자를 소문자로 변경하는 등의 전처리 작업을 수행한다.
- 토큰화 : 디폴트로 단어 기준으로 n_gram_range를 반영해 각 단어를 토큰화한다.
- 텍스트 정규화 : Stop Word 필터링을 수행한다. ( Stemmer, Lemmatize는 자체 지원 x )
- 피처 벡터화 : max_df, min_df, max_features 등의 파라미터를 이용해 토큰화된 단어를 피처로 추출하고 단어 빈도수 벡터 값을 적용한다.